{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915848dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Extracted 0 rows  â†’ outputs\\extracted\\excel_summary.json\n",
      "[OK] Log                        â†’ outputs\\extracted\\excel_summary_log.csv\n"
     ]
    }
   ],
   "source": [
    "# extract_excel_summary.py\n",
    "# Phase 2 - Step 4: Extract summary data from Excel-like calc files into JSON.\n",
    "# - Scans a root folder for .xlsx/.xlsm/.xls\n",
    "# - Finds \"summary-like\" tables (Actual/Allowable/RF + Component + Criterion + Output Set)\n",
    "# - Outputs: outputs/extracted/excel_summary.json + excel_summary_log.csv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "SHEET_NAME_HINTS = [\n",
    "    \"summary\", \"output\", \"results\", \"rf\", \"interaction\",\n",
    "    \"bearing\", \"buckling\", \"joint\", \"panel\"\n",
    "]\n",
    "SUPPORTED_EXT = {\".xlsx\", \".xlsm\", \".xls\"}\n",
    "\n",
    "KEYWORDS = {\n",
    "    \"component\": [\n",
    "        \"component\", \"panel\", \"panel id\", \"joint\", \"location\", \"item\",\n",
    "        \"part\", \"bay\", \"rib\", \"stringer\", \"upper panel\", \"lower panel\"\n",
    "    ],\n",
    "    \"criterion\": [\n",
    "        \"criterion\", \"criteria\", \"check\", \"mode\", \"failure\", \"type\",\n",
    "        \"buckling\", \"bearing\", \"net tension\", \"net-tension\",\n",
    "        \"shear-out\", \"crippling\", \"irb\", \"inter-rivet\", \"tsai-wu\",\n",
    "        \"interaction\"\n",
    "    ],\n",
    "    \"actual\": [\n",
    "        \"actual\", \"applied\", \"demand\", \"calc\", \"computed\", \"value\",\n",
    "        \"load\", \"stress\", \"flux\"\n",
    "    ],\n",
    "    \"allowable\": [\n",
    "        \"allowable\", \"limit\", \"capacity\", \"capability\", \"allow\",\n",
    "        \"ultimate\", \"ult\", \"allowable flux\", \"allowable stress\"\n",
    "    ],\n",
    "    \"rf\": [\n",
    "        \"rf\", \"reserve factor\", \"mos\", \"margin\"\n",
    "    ],\n",
    "    \"output_set\": [\n",
    "        \"output set id\", \"output set\", \"load case\", \"case\", \"pdf case id\", \"osid\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "def normalize(s: Any) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().lower()) if s is not None else \"\"\n",
    "\n",
    "def matches_any(cell_text: str, field: str) -> bool:\n",
    "    t = normalize(cell_text)\n",
    "    for kw in KEYWORDS[field]:\n",
    "        if kw in t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_candidate_sheet(name: str) -> bool:\n",
    "    n = normalize(name)\n",
    "    return any(h in n for h in SHEET_NAME_HINTS)\n",
    "\n",
    "def to_float(x) -> Optional[float]:\n",
    "    if x is None or (isinstance(x, str) and normalize(x) in {\"\", \"nan\", \"none\", \"-\"}):\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return float(x)\n",
    "    s = str(x).strip()\n",
    "    s = s.replace(\" \", \"\").replace(\",\", \".\")  # support decimal commas\n",
    "    s = re.sub(r\"[^0-9eE+\\-.]\", \"\", s)       # remove units/symbols\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------- openpyxl path for xlsx/xlsm (with cell addresses) --------\n",
    "def find_header_openpyxl(ws) -> Tuple[Optional[int], Dict[str, int]]:\n",
    "    max_scan_rows = min(25, ws.max_row)\n",
    "    for r in range(1, max_scan_rows + 1):\n",
    "        row_vals = [ws.cell(row=r, column=c).value for c in range(1, ws.max_column + 1)]\n",
    "        hits: Dict[str, int] = {}\n",
    "        for c, val in enumerate(row_vals, start=1):\n",
    "            if val is None: \n",
    "                continue\n",
    "            text = str(val)\n",
    "            for field in [\"component\", \"criterion\", \"actual\", \"allowable\", \"rf\", \"output_set\"]:\n",
    "                if matches_any(text, field):\n",
    "                    hits[field] = c - 1  # 0-based index\n",
    "        if len(hits) >= 3:\n",
    "            return r, hits\n",
    "    return None, {}\n",
    "\n",
    "def col_letter(idx_1based: int) -> str:\n",
    "    # Excel column index -> letter\n",
    "    s = \"\"\n",
    "    n = idx_1based\n",
    "    while n:\n",
    "        n, rem = divmod(n - 1, 26)\n",
    "        s = chr(65 + rem) + s\n",
    "    return s\n",
    "\n",
    "def extract_openpyxl(path: Path) -> List[Dict]:\n",
    "    wb = load_workbook(filename=str(path), data_only=True, read_only=True)\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    # Prefer candidate sheets, else scan all\n",
    "    sheets = [s for s in wb.sheetnames if is_candidate_sheet(s)]\n",
    "    if not sheets:\n",
    "        sheets = wb.sheetnames[:]\n",
    "\n",
    "    for sh in sheets:\n",
    "        ws = wb[sh]\n",
    "        header_row, colmap = find_header_openpyxl(ws)\n",
    "        if not header_row:\n",
    "            continue\n",
    "\n",
    "        r = header_row + 1\n",
    "        blank_streak = 0\n",
    "        while r <= ws.max_row and blank_streak < 2:\n",
    "            any_data = False\n",
    "            row_dict: Dict[str, Any] = {\n",
    "                \"component\": None, \"criterion\": None,\n",
    "                \"actual\": None, \"allowable\": None, \"rf\": None,\n",
    "                \"output_set_id\": None,\n",
    "                \"source_file\": str(path),\n",
    "                \"source_sheet\": sh,\n",
    "                \"source_cells\": {}\n",
    "            }\n",
    "            for field, c0 in colmap.items():\n",
    "                c = c0 + 1  # 1-based\n",
    "                val = ws.cell(row=r, column=c).value\n",
    "                if field in {\"actual\", \"allowable\", \"rf\"}:\n",
    "                    row_dict[field] = to_float(val)\n",
    "                elif field in {\"component\", \"criterion\"}:\n",
    "                    row_dict[field] = str(val).strip() if val is not None else None\n",
    "                elif field == \"output_set\":\n",
    "                    num = to_float(val)\n",
    "                    row_dict[\"output_set_id\"] = int(num) if num is not None and float(num).is_integer() else (str(val).strip() if val else None)\n",
    "                row_dict[\"source_cells\"][field] = f\"{col_letter(c)}{r}\"\n",
    "                if val not in (None, \"\", \"-\"):\n",
    "                    any_data = True\n",
    "\n",
    "            if not any_data:\n",
    "                blank_streak += 1\n",
    "                r += 1\n",
    "                continue\n",
    "            blank_streak = 0\n",
    "\n",
    "            # Compute RF if missing but possible\n",
    "            if row_dict.get(\"rf\") is None and row_dict.get(\"actual\") and row_dict.get(\"allowable\"):\n",
    "                a = row_dict[\"actual\"]; b = row_dict[\"allowable\"]\n",
    "                if a and b and a != 0:\n",
    "                    row_dict[\"rf\"] = b / a\n",
    "\n",
    "            # Accept if we at least have component and RF (or actual+allowable)\n",
    "            if (row_dict.get(\"component\") or row_dict.get(\"criterion\")) and \\\n",
    "               (row_dict.get(\"rf\") is not None or\n",
    "                (row_dict.get(\"actual\") is not None and row_dict.get(\"allowable\") is not None)):\n",
    "                results.append(row_dict)\n",
    "\n",
    "            r += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------- pandas fallback for legacy .xls (no cell addresses) --------\n",
    "def find_header_pandas(df: pd.DataFrame) -> Tuple[Optional[int], Dict[str, int]]:\n",
    "    max_scan_rows = min(25, len(df))\n",
    "    for i in range(max_scan_rows):\n",
    "        row = df.iloc[i]\n",
    "        hits: Dict[str, int] = {}\n",
    "        for c, val in enumerate(list(row)):\n",
    "            text = normalize(val)\n",
    "            if not text or text in {\"nan\", \"none\"}:\n",
    "                continue\n",
    "            for field in [\"component\", \"criterion\", \"actual\", \"allowable\", \"rf\", \"output_set\"]:\n",
    "                if matches_any(text, field):\n",
    "                    hits[field] = c\n",
    "        if len(hits) >= 3:\n",
    "            return i, hits\n",
    "    return None, {}\n",
    "\n",
    "def extract_pandas_xls(path: Path) -> List[Dict]:\n",
    "    try:\n",
    "        xls = pd.ExcelFile(path, engine=\"xlrd\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    results: List[Dict] = []\n",
    "    # Prefer candidate sheets\n",
    "    sheets = [s for s in xls.sheet_names if is_candidate_sheet(s)]\n",
    "    if not sheets:\n",
    "        sheets = xls.sheet_names[:]\n",
    "    for sh in sheets:\n",
    "        try:\n",
    "            df = pd.read_excel(xls, sheet_name=sh, header=None, dtype=str)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if df.empty:\n",
    "            continue\n",
    "        header_row, colmap = find_header_pandas(df)\n",
    "        if header_row is None:\n",
    "            continue\n",
    "        start = header_row + 1\n",
    "        blanked = 0\n",
    "        for r in range(start, len(df)):\n",
    "            row = list(df.iloc[r])\n",
    "            nonempty = sum(1 for v in row if pd.notna(v) and str(v).strip() != \"\")\n",
    "            if nonempty <= 1:\n",
    "                if results:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            item = {\n",
    "                \"component\": None, \"criterion\": None,\n",
    "                \"actual\": None, \"allowable\": None, \"rf\": None,\n",
    "                \"output_set_id\": None,\n",
    "                \"source_file\": str(path),\n",
    "                \"source_sheet\": sh,\n",
    "                \"source_cells\": None\n",
    "            }\n",
    "            for field, c in colmap.items():\n",
    "                val = row[c] if c < len(row) else None\n",
    "                if field in {\"actual\", \"allowable\", \"rf\"}:\n",
    "                    item[field] = to_float(val)\n",
    "                elif field in {\"component\", \"criterion\"}:\n",
    "                    item[field] = str(val).strip() if val else None\n",
    "                elif field == \"output_set\":\n",
    "                    num = to_float(val)\n",
    "                    item[\"output_set_id\"] = int(num) if num is not None and float(num).is_integer() else (str(val).strip() if val else None)\n",
    "            if item.get(\"rf\") is None and item.get(\"actual\") and item.get(\"allowable\") and item[\"actual\"] != 0:\n",
    "                item[\"rf\"] = item[\"allowable\"] / item[\"actual\"]\n",
    "            if (item.get(\"component\") or item.get(\"criterion\")) and \\\n",
    "               (item.get(\"rf\") is not None or (item.get(\"actual\") is not None and item.get(\"allowable\") is not None)):\n",
    "                results.append(item)\n",
    "    return results\n",
    "\n",
    "def crawl_and_extract(root: Path) -> Tuple[List[Dict], List[Dict]]:\n",
    "    records: List[Dict] = []\n",
    "    warnings: List[Dict] = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in SUPPORTED_EXT:\n",
    "            try:\n",
    "                if p.suffix.lower() in {\".xlsx\", \".xlsm\"}:\n",
    "                    recs = extract_openpyxl(p)\n",
    "                else:\n",
    "                    recs = extract_pandas_xls(p)  # .xls fallback\n",
    "                if not recs:\n",
    "                    warnings.append({\"file\": str(p), \"warning\": \"No summary-like table found\"})\n",
    "                else:\n",
    "                    records.extend(recs)\n",
    "            except Exception as e:\n",
    "                warnings.append({\"file\": str(p), \"warning\": f\"Error: {e}\"})\n",
    "    return records, warnings\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python extract_excel_summary.py <root_calc_folder> [<out_json_path>]\")\n",
    "        sys.exit(1)\n",
    "    root = Path(sys.argv[1]).expanduser().resolve()\n",
    "    out_json = Path(sys.argv[2]).resolve() if len(sys.argv) >= 3 else Path(\"outputs/extracted/excel_summary.json\")\n",
    "    out_log = out_json.with_name(\"excel_summary_log.csv\")\n",
    "\n",
    "    out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    records, warns = crawl_and_extract(root)\n",
    "\n",
    "    # simple de-dup\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for r in records:\n",
    "        key = (r.get(\"component\"), r.get(\"criterion\"), r.get(\"rf\"), r.get(\"source_file\"), r.get(\"source_sheet\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(r)\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unique, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    pd.DataFrame(warns or [{\"file\":\"\", \"warning\":\"\"}]).to_csv(out_log, index=False)\n",
    "\n",
    "    print(f\"[OK] Extracted {len(unique)} rows  â†’ {out_json}\")\n",
    "    print(f\"[OK] Log                        â†’ {out_log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this block instead of the \"main()\" function in your Notebook\n",
    "from pathlib import Path\n",
    "\n",
    "# --- DEFINE YOUR INPUT PATH HERE ---\n",
    "# Point this to the folder containing your .xlsx files\n",
    "input_folder = \"inputs/excel\" \n",
    "\n",
    "# --- DEFINE YOUR OUTPUT PATH HERE ---\n",
    "output_json = \"outputs/extracted/excel_summary.json\"\n",
    "\n",
    "# Run the extraction\n",
    "root_path = Path(input_folder).expanduser().resolve()\n",
    "out_json_path = Path(output_json).resolve()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "out_json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run the crawler (from your provided code)\n",
    "records, warns = crawl_and_extract(root_path)\n",
    "\n",
    "# Simple de-dup logic (from your provided code)\n",
    "seen = set()\n",
    "unique = []\n",
    "for r in records:\n",
    "    key = (r.get(\"component\"), r.get(\"criterion\"), r.get(\"rf\"), r.get(\"source_file\"), r.get(\"source_sheet\"))\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique.append(r)\n",
    "\n",
    "# Save to JSON\n",
    "with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unique, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Success! Extracted {len(unique)} rows from Excel files.\")\n",
    "print(f\"ðŸ“‚ Data saved to: {out_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8c03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ec334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_op2_sets.py\n",
    "# Phase 2 - Step 5: List available Output Set IDs (subcases) from OP2 files.\n",
    "# Output: outputs/extracted/op2_loads.json\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from pyNastran.op2.op2 import OP2\n",
    "\n",
    "def list_sets(op2_path: Path) -> List[int]:\n",
    "    op2 = OP2()\n",
    "    # Quiet reading; skip non-essential tables for speed\n",
    "    op2.read_op2(str(op2_path), combine=True, build_dataframe=False, skip_undefined_matrices=True)\n",
    "    # Collect subcase / output set IDs found in common result tables\n",
    "    set_ids = set()\n",
    "\n",
    "    # Try several result containers if present\n",
    "    candidates = [\n",
    "        getattr(op2, \"cquad4_stress\", None),\n",
    "        getattr(op2, \"ctria3_stress\", None),\n",
    "        getattr(op2, \"cbar_stress\", None),\n",
    "        getattr(op2, \"gpforce\", None),\n",
    "        getattr(op2, \"grid_point_forces\", None),\n",
    "        getattr(op2, \"displacements\", None),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c is None:\n",
    "            continue\n",
    "        # new-style objects expose ._subcase_id or .subcase_id; also ._times for transient\n",
    "        try:\n",
    "            # Pandas-free simple API: iterate keys (subcase->data)\n",
    "            for key in c.keys():\n",
    "                try:\n",
    "                    set_ids.add(int(key))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            # Fallback: ignore if container isn't dict-like\n",
    "            pass\n",
    "\n",
    "    return sorted(set_ids)\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python extract_op2_sets.py <op2_root_folder> [<out_json>]\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    root = Path(sys.argv[1]).expanduser().resolve()\n",
    "    out_json = Path(sys.argv[2]).resolve() if len(sys.argv) >= 3 else Path(\"outputs/extracted/op2_loads.json\")\n",
    "    out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    result: Dict[str, List[int]] = {}\n",
    "    for p in root.rglob(\"*.op2\"):\n",
    "        try:\n",
    "            result[str(p)] = list_sets(p)\n",
    "            print(f\"[OK] {p.name}: sets={result[str(p)]}\")\n",
    "        except Exception as e:\n",
    "            result[str(p)] = []\n",
    "            print(f\"[WARN] {p.name}: {e}\")\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    print(f\"[DONE] Wrote {out_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6655e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_load_case_map.py\n",
    "# Phase 2 - Step 6: Validate and emit the OutputSet -> PDF CaseID mapping JSON.\n",
    "# Inputs:\n",
    "#   - outputs/extracted/excel_summary.json (from Step 4)\n",
    "#   - outputs/extracted/op2_loads.json     (from Step 5)  [optional check]\n",
    "#   - inputs/load_case_map_seed.csv        (your manual seed)\n",
    "# Output:\n",
    "#   - outputs/extracted/load_case_map.json\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 4:\n",
    "        print(\"Usage: python build_load_case_map.py <excel_summary_json> <seed_csv> <out_json> [<op2_loads_json>]\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    excel_summary = Path(sys.argv[1]).resolve()\n",
    "    seed_csv      = Path(sys.argv[2]).resolve()\n",
    "    out_json      = Path(sys.argv[3]).resolve()\n",
    "    op2_json      = Path(sys.argv[4]).resolve() if len(sys.argv) >= 5 else None\n",
    "\n",
    "    out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load extracted Excel rows\n",
    "    with open(excel_summary, \"r\", encoding=\"utf-8\") as f:\n",
    "        rows = json.load(f)\n",
    "    excel_sets = sorted({ int(r[\"output_set_id\"]) for r in rows if r.get(\"output_set_id\") is not None and str(r[\"output_set_id\"]).isdigit() })\n",
    "\n",
    "    # Load optional OP2 sets for validation\n",
    "    op2_sets = set()\n",
    "    if op2_json and op2_json.exists():\n",
    "        with open(op2_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            op2_map = json.load(f)\n",
    "        for _, sets in op2_map.items():\n",
    "            for s in sets:\n",
    "                if isinstance(s, int) or (isinstance(s, str) and s.isdigit()):\n",
    "                    op2_sets.add(int(s))\n",
    "\n",
    "    # Load seed CSV mapping\n",
    "    seed = pd.read_csv(seed_csv).fillna(\"\")\n",
    "    mapping = {}\n",
    "    problems = []\n",
    "\n",
    "    for _, row in seed.iterrows():\n",
    "        osid = str(row[\"output_set_id\"]).strip()\n",
    "        pcid = str(row[\"pdf_case_id\"]).strip()\n",
    "        if not osid or not osid.isdigit() or not pcid or not pcid.isdigit():\n",
    "            problems.append(f\"Invalid row in seed: output_set_id={osid}, pdf_case_id={pcid}\")\n",
    "            continue\n",
    "        osid_i = int(osid)\n",
    "        mapping[osid] = pcid\n",
    "\n",
    "        # Optional: sanity checks\n",
    "        if osid_i not in excel_sets:\n",
    "            problems.append(f\"Seed maps OutputSet {osid_i}, but it's not seen in excel_summary.json\")\n",
    "        if op2_sets and osid_i not in op2_sets:\n",
    "            problems.append(f\"Seed maps OutputSet {osid_i}, but it's not seen in op2_loads.json\")\n",
    "\n",
    "    # Emit JSON\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] Wrote mapping â†’ {out_json}\")\n",
    "    if problems:\n",
    "        print(\"\\n[WARN] Validation notes:\")\n",
    "        for p in problems:\n",
    "            print(\" - \" + p)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
